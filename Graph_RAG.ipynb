{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install graphrag --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "python"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from graphrag.query.indexer_adapters import read_indexer_entities, read_indexer_reports\n",
    "from graphrag.query.llm.oai.chat_openai import ChatOpenAI\n",
    "from graphrag.query.llm.oai.typing import OpenaiApiType\n",
    "from graphrag.query.structured_search.global_search.community_context import (\n",
    "    GlobalCommunityContext,\n",
    ")\n",
    "from graphrag.query.structured_search.global_search.search import GlobalSearch\n",
    "import subprocess\n",
    "import util\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import os\n",
    "\n",
    "from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey\n",
    "from graphrag.query.indexer_adapters import (\n",
    "    read_indexer_covariates,\n",
    "    read_indexer_entities,\n",
    "    read_indexer_relationships,\n",
    "    read_indexer_reports,\n",
    "    read_indexer_text_units,\n",
    ")\n",
    "from graphrag.query.input.loaders.dfs import (\n",
    "    store_entity_semantic_embeddings,\n",
    ")\n",
    "from graphrag.query.llm.oai.chat_openai import ChatOpenAI\n",
    "from graphrag.query.llm.oai.embedding import OpenAIEmbedding\n",
    "from graphrag.query.llm.oai.typing import OpenaiApiType\n",
    "from graphrag.query.structured_search.local_search.mixed_context import (\n",
    "    LocalSearchMixedContext,\n",
    ")\n",
    "from graphrag.query.structured_search.local_search.search import LocalSearch\n",
    "from graphrag.vector_stores.lancedb import LanceDBVectorStore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all cross-reference text files\n",
    "source_folders = [\"requirements/FR\",\"requirements/NFR\",\"article_chunks\"]\n",
    "for f in source_folders:\n",
    "    source_folder = f'documents/{f}'\n",
    "    destination_folder = 'input'\n",
    "    output_file = 'articles.txt'\n",
    "\n",
    "    output_file_path = os.path.join(destination_folder, output_file)\n",
    "    all_text = []\n",
    "\n",
    "    for filename in os.listdir(source_folder):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(source_folder, filename)\n",
    "            \n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                all_text.append(file.read())\n",
    "\n",
    "    combined_text = '\\n'.join(all_text)\n",
    "\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.write(combined_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove uncommon characters \n",
    "def clean_text_file(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    cleaned_content = ''.join(char for char in content if ord(char) < 128)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(cleaned_content)\n",
    "input_file = \"input/articles.txt\"\n",
    "clean_text_file(input_file, input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command executed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Generate a Graph RAG from the provided text file. or you can run \"python -m graphrag.index --root .\" in terminal.\n",
    "# WARNING: Running this cell may incur significant costs depending on the size of your content and the model used.\n",
    "\n",
    "command = ['python', '-m', 'graphrag.index', '--root', '.']\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"Command executed successfully.\")\n",
    "else:\n",
    "    print(\"Command failed with return code\", result.returncode)\n",
    "    print(result.stderr)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moved files from output/****-****\n",
    "INPUT_DIR = \"output/graphs/GPT-UP/artifacts\"\n",
    "LANCEDB_URI = f\"{INPUT_DIR}/lancedb\"\n",
    "COMMUNITY_LEVEL = 2\n",
    "\n",
    "report_df = pd.read_parquet(f\"{INPUT_DIR}/create_final_community_reports.parquet\")\n",
    "entity_df = pd.read_parquet(f\"{INPUT_DIR}/create_final_nodes.parquet\")\n",
    "entity_embedding_df = pd.read_parquet(f\"{INPUT_DIR}/create_final_entities.parquet\")\n",
    "relationship_df = pd.read_parquet(f\"{INPUT_DIR}/create_final_relationships.parquet\")\n",
    "text_unit_df = pd.read_parquet(f\"{INPUT_DIR}/create_final_text_units.parquet\")\n",
    "entities = read_indexer_entities(entity_df, entity_embedding_df, COMMUNITY_LEVEL)\n",
    "text_units = read_indexer_text_units(text_unit_df)\n",
    "relationships = read_indexer_relationships(relationship_df)\n",
    "description_embedding_store = LanceDBVectorStore(\n",
    "    collection_name=\"entity_description_embeddings\",\n",
    ")\n",
    "description_embedding_store.connect(db_uri=LANCEDB_URI)\n",
    "entity_description_embeddings = store_entity_semantic_embeddings(\n",
    "    entities=entities, vectorstore=description_embedding_store\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   community                                       full_content  level  rank  \\\n",
      "0          0  # ISO/IEC 27001:2013(E) and Information Securi...      0   8.5   \n",
      "1          1  # ISO/IEC 27001:2013 Information Security Mana...      0   8.5   \n",
      "2         10  # Security in Development and Support Processe...      0   8.5   \n",
      "3          2  # Information Security Management Systems and ...      0   8.5   \n",
      "4          3  # ISO/IEC 27001:2013 and Related Standards Com...      0   8.5   \n",
      "5          4  # Cryptographic Controls and ISO/IEC 27001:201...      0   7.5   \n",
      "6          5  # ISO/IEC 27001:2013 and Information Security ...      0   8.5   \n",
      "7          6  # Human Resource Security in ISO/IEC 27001:201...      0   7.5   \n",
      "8          7  # Information Security Incident Management Com...      0   8.5   \n",
      "9          8  # Information Security Management System and I...      0   8.5   \n",
      "10         9  # ISO/IEC 2013 and Information Security Object...      0   8.5   \n",
      "\n",
      "                                                title  \\\n",
      "0   ISO/IEC 27001:2013(E) and Information Security...   \n",
      "1   ISO/IEC 27001:2013 Information Security Manage...   \n",
      "2       Security in Development and Support Processes   \n",
      "3   Information Security Management Systems and Ri...   \n",
      "4   ISO/IEC 27001:2013 and Related Standards Commu...   \n",
      "5       Cryptographic Controls and ISO/IEC 27001:2013   \n",
      "6   ISO/IEC 27001:2013 and Information Security Pr...   \n",
      "7       Human Resource Security in ISO/IEC 27001:2013   \n",
      "8   Information Security Incident Management Commu...   \n",
      "9   Information Security Management System and ISO...   \n",
      "10   ISO/IEC 2013 and Information Security Objectives   \n",
      "\n",
      "                                     rank_explanation  \\\n",
      "0   The impact severity rating is high due to the ...   \n",
      "1   The impact severity rating is high due to the ...   \n",
      "2   The impact severity rating is high due to the ...   \n",
      "3   The impact severity rating is high due to the ...   \n",
      "4   The impact severity rating is high due to the ...   \n",
      "5   The impact severity rating is high due to the ...   \n",
      "6   The impact severity rating is high due to the ...   \n",
      "7   The impact severity rating is high due to the ...   \n",
      "8   The impact severity rating is high due to the ...   \n",
      "9   The impact severity rating is high due to the ...   \n",
      "10  The impact severity rating is high due to the ...   \n",
      "\n",
      "                                              summary  \\\n",
      "0   The community is centered around the ISO/IEC 2...   \n",
      "1   The community is centered around the ISO/IEC 2...   \n",
      "2   The community is centered around the 'Security...   \n",
      "3   The community is centered around the Informati...   \n",
      "4   This community is centered around the ISO/IEC ...   \n",
      "5   The community is centered around the implement...   \n",
      "6   The community is centered around the ISO/IEC 2...   \n",
      "7   The community focuses on Human Resource Securi...   \n",
      "8   The community is centered around Information S...   \n",
      "9   The community is centered around the Informati...   \n",
      "10  The community is centered around the ISO/IEC 2...   \n",
      "\n",
      "                                             findings  \\\n",
      "0   [{'explanation': 'ISO/IEC 27001:2013(E) serves...   \n",
      "1   [{'explanation': 'ISO/IEC 27001:2013 serves as...   \n",
      "2   [{'explanation': 'The 'Security in Development...   \n",
      "3   [{'explanation': 'The Information Security Man...   \n",
      "4   [{'explanation': 'ISO/IEC 27001:2013 is the co...   \n",
      "5   [{'explanation': 'ISO/IEC 27001:2013 is a pivo...   \n",
      "6   [{'explanation': 'Documented information is a ...   \n",
      "7   [{'explanation': 'Human Resource Security is a...   \n",
      "8   [{'explanation': 'Information Security Inciden...   \n",
      "9   [{'explanation': 'The Information Security Man...   \n",
      "10  [{'explanation': 'ISO/IEC 2013 serves as a fou...   \n",
      "\n",
      "                                    full_content_json  \\\n",
      "0   {\\n    \"title\": \"ISO/IEC 27001:2013(E) and Inf...   \n",
      "1   {\\n    \"title\": \"ISO/IEC 27001:2013 Informatio...   \n",
      "2   {\\n    \"title\": \"Security in Development and S...   \n",
      "3   {\\n    \"title\": \"Information Security Manageme...   \n",
      "4   {\\n    \"title\": \"ISO/IEC 27001:2013 and Relate...   \n",
      "5   {\\n    \"title\": \"Cryptographic Controls and IS...   \n",
      "6   {\\n    \"title\": \"ISO/IEC 27001:2013 and Inform...   \n",
      "7   {\\n    \"title\": \"Human Resource Security in IS...   \n",
      "8   {\\n    \"title\": \"Information Security Incident...   \n",
      "9   {\\n    \"title\": \"Information Security Manageme...   \n",
      "10  {\\n    \"title\": \"ISO/IEC 2013 and Information ...   \n",
      "\n",
      "                                      id  \n",
      "0   5d073e78-069d-42fa-9eef-4ef17f111538  \n",
      "1   bc3d8e11-f39b-4076-8901-f08dff309ee2  \n",
      "2   a8d3de8b-0c4e-476d-840a-8f49fbe7a3ed  \n",
      "3   46019e22-0b54-4e41-986f-0091ae91224e  \n",
      "4   f0a307be-006c-4b3e-bfda-87edd567eca6  \n",
      "5   9585e8aa-fcc2-47f4-bda0-c8e673b938c1  \n",
      "6   9edd5794-3d26-4fd7-b032-e82b741633e0  \n",
      "7   c5da3d02-9dd8-4457-b1e3-d3d55250259a  \n",
      "8   ed224529-936c-4e14-a4b3-2b9d7f5f735a  \n",
      "9   c0b0715e-e5a7-413a-8b7d-8dfcd3f01e52  \n",
      "10  c15be2df-0f6e-479e-a2e5-f6618fd43c36  \n"
     ]
    }
   ],
   "source": [
    "print(report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ[\"GRAPHRAG_API_KEY\"]\n",
    "llm_model = \"gpt-4o-2024-08-06\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    api_key=api_key,\n",
    "    model=llm_model,\n",
    "    api_type=OpenaiApiType.OpenAI,  \n",
    "    max_retries=20,\n",
    ")\n",
    "token_encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "reports = read_indexer_reports(report_df, entity_df, 2)\n",
    "entities = read_indexer_entities(entity_df, entity_embedding_df, 2)\n",
    "context_builder = GlobalCommunityContext(\n",
    "    community_reports=reports,\n",
    "    entities=entities,\n",
    "    token_encoder=token_encoder,\n",
    ")\n",
    "\n",
    "context_builder_params = {\n",
    "    \"use_community_summary\": False,  # False means using full community reports. True means using community short summaries.\n",
    "    \"shuffle_data\": True,\n",
    "    \"include_community_rank\": True,\n",
    "    \"min_community_rank\": 0,\n",
    "    \"community_rank_name\": \"rank\",\n",
    "    \"include_community_weight\": True,\n",
    "    \"community_weight_name\": \"occurrence weight\",\n",
    "    \"normalize_community_weight\": True,\n",
    "    \"max_tokens\": 5000,\n",
    "    \"context_name\": \"Reports\",\n",
    "}\n",
    "\n",
    "map_llm_params = {\n",
    "    \"max_tokens\": 1000,\n",
    "    \"temperature\": 0.0,\n",
    "    \"response_format\": {\"type\": \"json_object\"},\n",
    "}\n",
    "\n",
    "reduce_llm_params = {\n",
    "    \"max_tokens\": 1000, \n",
    "    \"temperature\": 0.0,\n",
    "}\n",
    "\n",
    "search_engine = GlobalSearch(\n",
    "    llm=llm,\n",
    "    context_builder=context_builder,\n",
    "    token_encoder=token_encoder,\n",
    "    max_data_tokens=5000, \n",
    "    map_llm_params=map_llm_params,\n",
    "    reduce_llm_params=reduce_llm_params,\n",
    "    allow_general_knowledge=False,  # set this to True will add instruction to encourage the LLM to incorporate general knowledge in the response, which may increase hallucinations, but could be useful in some use cases.\n",
    "    json_mode=True,  # set this to False if your LLM model does not support JSON mode.\n",
    "    context_builder_params=context_builder_params,\n",
    "    concurrent_coroutines=32,\n",
    "    response_type=\"multiple paragraphs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written content to output/search_results/GPT-OP\\req_1.txt\n",
      "Written content to output/search_results/GPT-OP\\req_10.txt\n",
      "Written content to output/search_results/GPT-OP\\req_11.txt\n",
      "Written content to output/search_results/GPT-OP\\req_12.txt\n",
      "Written content to output/search_results/GPT-OP\\req_13.txt\n",
      "Written content to output/search_results/GPT-OP\\req_14.txt\n",
      "Written content to output/search_results/GPT-OP\\req_15.txt\n",
      "Written content to output/search_results/GPT-OP\\req_19.txt\n",
      "Written content to output/search_results/GPT-OP\\req_2.txt\n",
      "Written content to output/search_results/GPT-OP\\req_20.txt\n",
      "Written content to output/search_results/GPT-OP\\req_21.txt\n",
      "Written content to output/search_results/GPT-OP\\req_23.txt\n",
      "Written content to output/search_results/GPT-OP\\req_24.txt\n",
      "Written content to output/search_results/GPT-OP\\req_26.txt\n",
      "Written content to output/search_results/GPT-OP\\req_27.txt\n",
      "Written content to output/search_results/GPT-OP\\req_28.txt\n",
      "Written content to output/search_results/GPT-OP\\req_29.txt\n",
      "Written content to output/search_results/GPT-OP\\req_30.txt\n",
      "Written content to output/search_results/GPT-OP\\req_31.txt\n",
      "Written content to output/search_results/GPT-OP\\req_32.txt\n",
      "Written content to output/search_results/GPT-OP\\req_33.txt\n",
      "Written content to output/search_results/GPT-OP\\req_34.txt\n",
      "Written content to output/search_results/GPT-OP\\req_35.txt\n",
      "Written content to output/search_results/GPT-OP\\req_36.txt\n",
      "Written content to output/search_results/GPT-OP\\req_38.txt\n",
      "Written content to output/search_results/GPT-OP\\req_39.txt\n",
      "Written content to output/search_results/GPT-OP\\req_4.txt\n",
      "Written content to output/search_results/GPT-OP\\req_6.txt\n",
      "Written content to output/search_results/GPT-OP\\req_9.txt\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'documents/requirements/FR'\n",
    "output_folder_path = 'output/search_results/GPT-OP'\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r') as file:\n",
    "            req_content = file.read()\n",
    "            result = await search_engine.asearch(f\"Identify content that are relevant to ensuring compliance or completeness of the following requirement: {req_content}\")\n",
    "    \n",
    "            output_file_path = os.path.join(output_folder_path, filename)\n",
    "            with open(output_file_path, 'w') as output_file:\n",
    "                output_file.write(result.response)\n",
    "                print(f\"Written content to {output_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>occurrence weight</th>\n",
       "      <th>content</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>39</td>\n",
       "      <td>ISO/IEC 27001:2013 Information Security Community</td>\n",
       "      <td>1.000000</td>\n",
       "      <td># ISO/IEC 27001:2013 Information Security Comm...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>23</td>\n",
       "      <td>Information Security Management Community</td>\n",
       "      <td>0.352941</td>\n",
       "      <td># Information Security Management Community\\n\\...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30</td>\n",
       "      <td>Documented Information and Information Securit...</td>\n",
       "      <td>0.205882</td>\n",
       "      <td># Documented Information and Information Secur...</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>31</td>\n",
       "      <td>ISO/IEC 2013 Information Security Management C...</td>\n",
       "      <td>0.176471</td>\n",
       "      <td># ISO/IEC 2013 Information Security Management...</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21</td>\n",
       "      <td>Broker Back-Office Application Standards and C...</td>\n",
       "      <td>0.176471</td>\n",
       "      <td># Broker Back-Office Application Standards and...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>25</td>\n",
       "      <td>ISO Standards for Risk Management and Informat...</td>\n",
       "      <td>0.147059</td>\n",
       "      <td># ISO Standards for Risk Management and Inform...</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>Information Security Risk Management Community</td>\n",
       "      <td>0.117647</td>\n",
       "      <td># Information Security Risk Management Communi...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>22</td>\n",
       "      <td>Regulatory Compliance and Reporting Standards</td>\n",
       "      <td>0.117647</td>\n",
       "      <td># Regulatory Compliance and Reporting Standard...</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>26</td>\n",
       "      <td>Broker Back Office System Compliance and Funct...</td>\n",
       "      <td>0.088235</td>\n",
       "      <td># Broker Back Office System Compliance and Fun...</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>36</td>\n",
       "      <td>ISO/IEC Standards for Information Security Man...</td>\n",
       "      <td>0.088235</td>\n",
       "      <td># ISO/IEC Standards for Information Security M...</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                              title  occurrence weight  \\\n",
       "25  39  ISO/IEC 27001:2013 Information Security Community           1.000000   \n",
       "26  23          Information Security Management Community           0.352941   \n",
       "8   30  Documented Information and Information Securit...           0.205882   \n",
       "17  31  ISO/IEC 2013 Information Security Management C...           0.176471   \n",
       "9   21  Broker Back-Office Application Standards and C...           0.176471   \n",
       "18  25  ISO Standards for Risk Management and Informat...           0.147059   \n",
       "33   1     Information Security Risk Management Community           0.117647   \n",
       "19  22      Regulatory Compliance and Reporting Standards           0.117647   \n",
       "10  26  Broker Back Office System Compliance and Funct...           0.088235   \n",
       "27  36  ISO/IEC Standards for Information Security Man...           0.088235   \n",
       "\n",
       "                                              content  rank  \n",
       "25  # ISO/IEC 27001:2013 Information Security Comm...   8.5  \n",
       "26  # Information Security Management Community\\n\\...   8.0  \n",
       "8   # Documented Information and Information Secur...   7.5  \n",
       "17  # ISO/IEC 2013 Information Security Management...   7.5  \n",
       "9   # Broker Back-Office Application Standards and...   8.5  \n",
       "18  # ISO Standards for Risk Management and Inform...   7.5  \n",
       "33  # Information Security Risk Management Communi...   8.0  \n",
       "19  # Regulatory Compliance and Reporting Standard...   8.5  \n",
       "10  # Broker Back Office System Compliance and Fun...   7.5  \n",
       "27  # ISO/IEC Standards for Information Security M...   7.5  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional: To get more context\n",
    "df = pd.DataFrame(result.context_data['reports'])\n",
    "context_df = df.sort_values(by='occurrence weight', ascending=False) \n",
    "filtered_df = context_df[context_df['occurrence weight'] > 0.7]\n",
    "result_str =  result.response\n",
    "context_df.head(10)\n",
    "\n",
    "content_context = context_df[\"content\"][0] + \"\\n\" + result_str\n",
    "sample_string = \"\"\n",
    "for resp in result.map_responses:\n",
    "    sample_string += str(resp)\n",
    "\n",
    "pattern = re.compile(r\"'answer':\\s*'(.*?)',\\s*'score':\\s*(\\d+)\")\n",
    "matches = pattern.findall(sample_string)\n",
    "\n",
    "df = pd.DataFrame(matches, columns=['Answer', 'Score'])\n",
    "df['Score'] = pd.to_numeric(df['Score'])\n",
    "sorted_df = df.sort_values(by='Score', ascending=False) \n",
    "\n",
    "filtered_df = df[df['Score'] >= 0.8]\n",
    "high_score_answer = '\\n'.join(filtered_df['Answer'])\n",
    "result_str = result_str + \"\\n\\n\" + high_score_answer\n",
    "result_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"text-embedding-3-small\"\n",
    "llm = ChatOpenAI(\n",
    "    api_key=api_key,\n",
    "    model=llm_model,\n",
    "    api_type=OpenaiApiType.OpenAI,  # OpenaiApiType.OpenAI or OpenaiApiType.AzureOpenAI\n",
    "    max_retries=20,\n",
    ")\n",
    "\n",
    "token_encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "text_embedder = OpenAIEmbedding(\n",
    "    api_key=api_key,\n",
    "    api_base=None,\n",
    "    api_type=OpenaiApiType.OpenAI,\n",
    "    model=embedding_model,\n",
    "    deployment_name=embedding_model,\n",
    "    max_retries=20,\n",
    ")\n",
    "\n",
    "context_builder = LocalSearchMixedContext(\n",
    "    community_reports=reports,\n",
    "    text_units=text_units,\n",
    "    entities=entities,\n",
    "    relationships=relationships,\n",
    "    # if you did not run covariates during indexing, set this to None\n",
    "    #covariates=covariates,\n",
    "    entity_text_embeddings=description_embedding_store,\n",
    "    embedding_vectorstore_key=EntityVectorStoreKey.ID,  # if the vectorstore uses entity title as ids, set this to EntityVectorStoreKey.TITLE\n",
    "    text_embedder=text_embedder,\n",
    "    token_encoder=token_encoder,\n",
    ")\n",
    "\n",
    "local_context_params = {\n",
    "    \"text_unit_prop\": 0.5,\n",
    "    \"community_prop\": 0.1,\n",
    "    \"conversation_history_max_turns\": 5,\n",
    "    \"conversation_history_user_turns_only\": True,\n",
    "    \"top_k_mapped_entities\": 10,\n",
    "    \"top_k_relationships\": 10,\n",
    "    \"include_entity_rank\": True,\n",
    "    \"include_relationship_weight\": True,\n",
    "    \"include_community_rank\": False,\n",
    "    \"return_candidate_context\": False,\n",
    "    \"embedding_vectorstore_key\": EntityVectorStoreKey.ID,  # set this to EntityVectorStoreKey.TITLE if the vectorstore uses entity title as ids\n",
    "    \"max_tokens\": 12_000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000)\n",
    "}\n",
    "\n",
    "llm_params = {\n",
    "    \"max_tokens\": 2_000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 1000=1500)\n",
    "    \"temperature\": 0.0,\n",
    "}\n",
    "\n",
    "search_engine = LocalSearch(\n",
    "    llm=llm,\n",
    "    context_builder=context_builder,\n",
    "    token_encoder=token_encoder,\n",
    "    llm_params=llm_params,\n",
    "    context_builder_params=local_context_params,\n",
    "    response_type=\"multiple paragraphs\",  # free form text describing the response type and format, can be anything, e.g. prioritized list, single paragraph, multiple paragraphs, multiple-page report\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written content to output/search_results/GPT-OP\\req_1.txt\n",
      "Written content to output/search_results/GPT-OP\\req_10.txt\n",
      "Written content to output/search_results/GPT-OP\\req_11.txt\n",
      "Written content to output/search_results/GPT-OP\\req_12.txt\n",
      "Written content to output/search_results/GPT-OP\\req_13.txt\n",
      "Written content to output/search_results/GPT-OP\\req_14.txt\n",
      "Written content to output/search_results/GPT-OP\\req_15.txt\n",
      "Written content to output/search_results/GPT-OP\\req_19.txt\n",
      "Written content to output/search_results/GPT-OP\\req_2.txt\n",
      "Written content to output/search_results/GPT-OP\\req_20.txt\n",
      "Written content to output/search_results/GPT-OP\\req_21.txt\n",
      "Written content to output/search_results/GPT-OP\\req_23.txt\n",
      "Written content to output/search_results/GPT-OP\\req_24.txt\n",
      "Written content to output/search_results/GPT-OP\\req_26.txt\n",
      "Written content to output/search_results/GPT-OP\\req_27.txt\n",
      "Written content to output/search_results/GPT-OP\\req_28.txt\n",
      "Written content to output/search_results/GPT-OP\\req_29.txt\n",
      "Written content to output/search_results/GPT-OP\\req_30.txt\n",
      "Written content to output/search_results/GPT-OP\\req_31.txt\n",
      "Written content to output/search_results/GPT-OP\\req_32.txt\n",
      "Written content to output/search_results/GPT-OP\\req_33.txt\n",
      "Written content to output/search_results/GPT-OP\\req_34.txt\n",
      "Written content to output/search_results/GPT-OP\\req_35.txt\n",
      "Written content to output/search_results/GPT-OP\\req_36.txt\n",
      "Written content to output/search_results/GPT-OP\\req_38.txt\n",
      "Written content to output/search_results/GPT-OP\\req_39.txt\n",
      "Written content to output/search_results/GPT-OP\\req_4.txt\n",
      "Written content to output/search_results/GPT-OP\\req_6.txt\n",
      "Written content to output/search_results/GPT-OP\\req_9.txt\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'documents/requirements/FR'\n",
    "output_folder_path = 'output/search_results/GPT-OP'\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r') as file:\n",
    "            req_content = file.read()\n",
    "            result = await search_engine.asearch(f\"Identify content that are relevant to ensuring compliance or completeness of the following requirement: {req_content}\")\n",
    "    \n",
    "            output_file_path = os.path.join(output_folder_path, filename)\n",
    "            with open(output_file_path, 'a') as output_file:\n",
    "                output_file.write(result.response)\n",
    "                print(f\"Written content to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Account balance\\n\\nError Handling: If the account balance is not retrieved within a specified time, a new balance request is sent, and an appropriate error message is displayed. The error event is logged for further review.\\nTitle  Transferring Funds for Selling Stocks or Bonds  \\nDescription  The amount corresponding to the sold stocks is deposited into the customer's account, and \\nthe amount is withdrawn from the buyer's account. The brokerage fee is also withdrawn \\nfrom the buyer's account and deposited into the brokerage account.  \\nInput  Customer account number, stock or bond amount, brokerage fee, buyer's account number, \\nbrokerage account number  \\nProcessing  During the sale process, the user selects one of their accounts. The account balance is \\nchecked, and the system sets the amount for the sold stocks and the brokerage fee. The \\nbrokerage account is recorded in the system's database and needs only to be retrieved. The \\nstock exchange interface specifies the bank account of the buyer, and the funds are \\ntransferred accordingly.  \\nOutput  Increase in customer account balance by the stock amount, decrease in customer account \\nbalance by the brokerage fee amount, decrease in buyer's account balance by the stock \\namount, increase in brokerage account balance by the brokerage fee amount.  \\nError Handling  If there is an error in setting the account numbers, amounts, or if the stock exchange \\ninterface does not specify the buyer's account number, an appropriate error message is \\ndisplayed.\\n\\nTitle: Blocking Funds\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retrieving the original reference text\n",
    "result.context_data[\"sources\"][\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find redundant or contradicting solutions. (self consistency check)\n",
    "# directory = 'RAG_OUTPUT/SOLUTIONS/'\n",
    "# text_files = [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "# result = \"\"\n",
    "# if len(text_files) > 1:\n",
    "#     for i in range(len(text_files)):\n",
    "#         for j in range(i + 1, len(text_files)):\n",
    "#             file1_path = os.path.join(directory, text_files[i])\n",
    "#             file2_path = os.path.join(directory, text_files[j])\n",
    "#             with open(file1_path, 'r', encoding='utf-8') as file1:\n",
    "#                 file1_contents = file1.read()\n",
    "\n",
    "#             with open(file2_path, 'r', encoding='utf-8') as file2:\n",
    "#                 file2_contents = file2.read()\n",
    "#             combined_contents = file1_contents + \"\\n\" + file2_contents\n",
    "#             response = call_LLM(prompt_template.prompt_user_self_consistency(combined_contents), system_prompt = prompt_template.prompt_consistency(\"\"))\n",
    "#             result += response.choices[0].message.content\n",
    "# else:\n",
    "#     file_path = os.path.join(directory, \"sol_1.txt\")\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         file_contents = file.read()\n",
    "#         response = call_LLM(prompt_template.prompt_user_self_consistency(file_contents), system_prompt = prompt_template.prompt_consistency(\"\"))\n",
    "#         result = response.choices[0].message.content\n",
    "\n",
    "# with open(f'RAG_OUTPUT/CONSISTENCY/SELF_CON/self_con_result_{req_num}.txt', 'w', encoding='utf-8') as confile:\n",
    "#     confile.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df for self consistencies\n",
    "# user_input = []\n",
    "# content_from = []\n",
    "# similarity_score = []\n",
    "# similarity_number = []\n",
    "\n",
    "# with open(f'RAG_OUTPUT/CONSISTENCY/SELF_CON/self_con_result_{req_num}.txt', 'r') as file:\n",
    "#     lines = file.readlines()\n",
    "\n",
    "# for line in lines:\n",
    "#     if \"Similarity #\" in line:\n",
    "#         sim_number = int(re.search(r'\\d+', line).group())\n",
    "#         similarity_number.append(sim_number)\n",
    "#     elif \"(suggested solution):\" in line:\n",
    "#         user_input.append(int(re.search(r'\\d+', line).group()))\n",
    "#     elif \"Content from :\" in line:\n",
    "#         content_from.append(int(re.search(r'\\d+', line).group()))\n",
    "#     elif \"Similarity Score:\" in line:\n",
    "#         similarity_score.append(int(re.search(r'\\d+', line).group()))\n",
    "\n",
    "# df = pd.DataFrame({\n",
    "#     'Similarity Number': similarity_number,\n",
    "#     'User Input (Suggested Solution)': user_input,\n",
    "#     'Content From': content_from,\n",
    "#     'Similarity Score': similarity_score\n",
    "# })\n",
    "\n",
    "\n",
    "# df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove redundant suggestions. conflicts are manually handled.\n",
    "# content_from_list = df[df['Similarity Score'] >= 70]['Content From'].tolist()\n",
    "# num_of_self_conflicts = len(df[df['Similarity Score'] == 0]['Content From'].tolist())\n",
    "# num_of_self_redundencies = len(content_from_list)\n",
    "\n",
    "# number_of_sol_files = len([f for f in os.listdir(\"RAG_OUTPUT/SOLUTIONS\") if f.endswith('.txt')])\n",
    "\n",
    "\n",
    "# for i in range(1,number_of_sol_files + 1):\n",
    "#     with open(f'RAG_OUTPUT/SOLUTIONS/sol_{i}.txt', 'r') as file:\n",
    "#         lines = file.readlines()\n",
    "\n",
    "#         filtered_lines = [line for index, line in enumerate(lines, start=1) if index not in content_from_list]\n",
    "#         with open(f'RAG_OUTPUT/SOLUTIONS/update/updated_sol_{i}.txt', 'w') as file:\n",
    "#             file.writelines(filtered_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # consistency check between remaining solutions and existing requirement document.\n",
    "\n",
    "# req_directory = 'documents/requirements/'\n",
    "# sol_directory = 'RAG_OUTPUT/SOLUTIONS/update/'\n",
    "\n",
    "# sol_files = [f for f in os.listdir(sol_directory) if f.endswith('.txt')]\n",
    "# req_files = [f for f in os.listdir(req_directory) if f.endswith('.txt')]\n",
    "# result = \"\"\n",
    "\n",
    "# for i in range(len(sol_files)):\n",
    "#     for j in range(len(req_files)):\n",
    "#         sol_path = os.path.join(sol_directory, sol_files[i])\n",
    "#         req_path = os.path.join(req_directory, req_files[j])\n",
    "#         with open(sol_path, 'r', encoding='utf-8') as file1:\n",
    "#             sol_contents = file1.read()\n",
    "#         with open(req_path, 'r', encoding='utf-8') as file2:\n",
    "#             req_contents = file2.read()\n",
    "#         response = call_LLM(prompt_template.prompt_user_consistency(sol_contents), system_prompt = prompt_template.prompt_consistency(req_contents))\n",
    "#         result += response.choices[0].message.content\n",
    "\n",
    "# with open(f\"RAG_OUTPUT/CONSISTENCY/con_res_{req_num}.txt\", 'w', encoding='utf-8') as confile:\n",
    "#     confile.write(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df for consistencies\n",
    "# user_input = []\n",
    "# content_from = []\n",
    "# similarity_score = []\n",
    "# similarity_number = []\n",
    "\n",
    "# with open(f\"RAG_OUTPUT/CONSISTENCY/con_res_{req_num}.txt\", 'r') as file:\n",
    "#     lines = file.readlines()\n",
    "\n",
    "# for line in lines:\n",
    "#     if \"Similarity #\" in line or \"Contradiction #\" in line:\n",
    "#         sim_number = int(re.search(r'\\d+', line).group())\n",
    "#         similarity_number.append(sim_number)\n",
    "#     elif \"(suggested solution):\" in line:\n",
    "#         user_input.append(int(re.search(r'\\d+', line).group()))\n",
    "#     elif \"Content from \" in line:\n",
    "#         content_from.append(int(re.search(r'\\d+', line).group()))\n",
    "#     elif \"Similarity Score:\" in line:\n",
    "#         similarity_score.append(int(re.search(r'\\d+', line).group()))\n",
    "# df = pd.DataFrame({\n",
    "#     'Similarity Number': similarity_number,\n",
    "#     'User Input (Suggested Solution)': user_input,\n",
    "#     'Content From': content_from,\n",
    "#     'Similarity Score': similarity_score\n",
    "# })\n",
    "\n",
    "# content_from_list = df[df['Similarity Score'] >= 70]['Content From'].tolist()\n",
    "# num_of_conflicts = len(df[df['Similarity Score'] == 0]['Content From'].tolist()) - num_of_self_conflicts\n",
    "# num_of_redundencies = len(content_from_list) - num_of_self_redundencies\n",
    "\n",
    "# df.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
