{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "import asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_28072\\2312871627.py:108: RuntimeWarning: coroutine 'amain' was never awaited\n",
      "  loop.create_task(amain())\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "BERT model 'bert-base-uncased' and tokenizer loaded successfully.\n",
      "Loaded 2318 documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding documents: 100%|██████████| 73/73 [00:23<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 2318 documents.\n",
      "Written content to output/search_results/BERT\\req_16.txt\n",
      "Written content to output/search_results/BERT\\req_17.txt\n",
      "Written content to output/search_results/BERT\\req_18.txt\n",
      "Written content to output/search_results/BERT\\req_22.txt\n",
      "Written content to output/search_results/BERT\\req_25.txt\n",
      "Written content to output/search_results/BERT\\req_3.txt\n",
      "Written content to output/search_results/BERT\\req_40.txt\n",
      "Written content to output/search_results/BERT\\req_5.txt\n",
      "Written content to output/search_results/BERT\\req_7.txt\n",
      "Written content to output/search_results/BERT\\req_8.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class BERTRetriever:\n",
    "    def __init__(self, model_name: str = 'bert-base-uncased', max_length: int = 512):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name).to(self.device)\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        print(f\"BERT model '{model_name}' and tokenizer loaded successfully.\")\n",
    "        \n",
    "        self.documents = []\n",
    "        self.document_embeddings = None\n",
    "\n",
    "    def load_documents(self, file_path: str):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            self.documents = [doc.strip() for doc in file.readlines() if doc.strip()]\n",
    "        print(f\"Loaded {len(self.documents)} documents.\")\n",
    "\n",
    "    def encode_text(self, text: str) -> np.ndarray:\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, \n",
    "                                padding='max_length', truncation=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "    def encode_documents(self, batch_size: int = 32):\n",
    "        embeddings = []\n",
    "        for i in tqdm(range(0, len(self.documents), batch_size), desc=\"Encoding documents\"):\n",
    "            batch = self.documents[i:i+batch_size]\n",
    "            batch_embeddings = self.encode_text(\" \".join(batch))\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        self.document_embeddings = np.vstack(embeddings)\n",
    "        print(f\"Encoded {len(self.documents)} documents.\")\n",
    "\n",
    "    def retrieve_documents(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "        query_embedding = self.encode_text(query)\n",
    "        similarities = cosine_similarity(query_embedding, self.document_embeddings)[0]\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "        return [(self.documents[i], similarities[i]) for i in top_indices]\n",
    "\n",
    "    def evaluate_retrieval(self, queries: List[str], relevant_docs: List[List[int]], \n",
    "                           k: int = 5) -> Tuple[float, float]:\n",
    "        precisions, recalls = [], []\n",
    "        for query, relevant in zip(queries, relevant_docs):\n",
    "            retrieved = self.retrieve_documents(query, top_k=k)\n",
    "            retrieved_indices = [self.documents.index(doc) for doc, _ in retrieved]\n",
    "            \n",
    "            true_positives = len(set(retrieved_indices) & set(relevant))\n",
    "            precision = true_positives / k\n",
    "            recall = true_positives / len(relevant)\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "        \n",
    "        avg_precision = sum(precisions) / len(precisions)\n",
    "        avg_recall = sum(recalls) / len(recalls)\n",
    "        return avg_precision, avg_recall\n",
    "    \n",
    "    async def aretrieve_documents(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "        query_embedding = self.encode_text(query)\n",
    "        similarities = cosine_similarity(query_embedding, self.document_embeddings)[0]\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "        return [(self.documents[i], similarities[i]) for i in top_indices]\n",
    "\n",
    "    async def process_requirements(self, input_folder: str, output_folder: str):\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        for filename in os.listdir(input_folder):\n",
    "            if filename.endswith('.txt'):\n",
    "                input_file_path = os.path.join(input_folder, filename)\n",
    "                with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "                    req_content = file.read().strip()\n",
    "                \n",
    "                query = f\"Identify content that are relevant to ensuring compliance or completeness of the following requirement: {req_content}\"\n",
    "                results = await self.aretrieve_documents(query)\n",
    "                \n",
    "                output_file_path = os.path.join(output_folder, filename)\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                    output_file.write(f\"Query: {query}\\n\\nRelevant Documents:\\n\")\n",
    "                    for doc, score in results:\n",
    "                        output_file.write(f\"Score: {score:.4f}\\nDocument: {doc}\\n\\n\")\n",
    "                \n",
    "                print(f\"Written content to {output_file_path}\")\n",
    "\n",
    "\n",
    "def load_json_file(file_path: str) -> dict:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "async def amain():\n",
    "    retriever = BERTRetriever()\n",
    "    retriever.load_documents('input/articles.txt')\n",
    "    retriever.encode_documents()\n",
    "\n",
    "    input_folder = \"documents/requirements/FR/modified\"\n",
    "    output_folder = \"output/search_results/BERT\"\n",
    "    await retriever.process_requirements(input_folder, output_folder)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        asyncio.run(amain())\n",
    "    except RuntimeError:\n",
    "        # If we're in an environment with an existing event loop\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            loop.create_task(amain())\n",
    "        else:\n",
    "            loop.run_until_complete(amain())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
